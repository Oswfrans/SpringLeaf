import pandas as pd
from pandas import *
import numpy as np
#from sklearn.decomposition import PCA
import sklearn.decomposition as deco
from sklearn.feature_selection import VarianceThreshold
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn import linear_model, decomposition
import matplotlib.pyplot as plt

#ribbit
csv_file='C:\Users\oswin.frans\Desktop\Python\Springleaf_train.csv'
#tp=pd.read_csv(csv_file, sep=',', iterator=True, chunksize=1000, low_memory=False)
#df = concat(tp, ignore_index=True)
#way to read all data

df=pd.read_csv(csv_file, sep=',', nrows=200)
#read part of the data
#delete rows if a certain column value is zero
#df = df[df.line_race != 0]
#df.to_csv('C:\Users\oswin.frans\Desktop\Springleaf_blaajhk.csv')
print "Data Read"

#first step port these 200 rows back to excel/stata and see how to clean it
#df.to_excel('C:\Users\oswin.frans\Desktop\Check.xlsx',sheet_name='sheet1', index=False)
#clean it
#Recode 1,5; 200, 202,216 ,222,237,274, 284,305, 325, 342,352, 353, 354,404,466,467,493(-1 is missing)
#VAR_218 and VAR_240 are missing
#221, 236, 272, 282, 303, 323, 340,350,351,352,402,464,465,491,1932
torecode=[5,200,202,216 ,221, 236, 272, 282, 303, 323, 340,350,351,352,402,464,465,491,1932]
#torecode=[5,200,202,216 ,222,237,274, 284,305, 325, 342,352, 353, 354,404,466,467,493,1934]
dummyall=pd.DataFrame()
#dummyal= dummyall.fillna(0)

for i in torecode:
	dumcat='dumcat'+str(i)+'.'
	#dumcat2='dumcat'+str(i)+'.2'
	dummy=pd.get_dummies(df.iloc[:,i], prefix=dumcat)
	#df=df.join(dummy.ix[:, dumcat2:])
	#print i
	#print dummy.head()
	dummyall=dummyall.append(dummy)

#print dummyall.head()	
df=df.join(dummyall)
print "Joined"

#timestamp 75, 204, 217

#timestamp with a lot of missing values 73, 156 t/m 159, 166 t/m 169, 176 t/m 179

dTimebin=pd.DataFrame()

dtmz=[73, 75, 156,157,158,159,166,167,168,169,176,177,178,179,204,217]

for i in dtmz:
	dTimebin=dTimebin.join(df.iloc[:,i])

#some misssing values 74, 205 t/m 211

#Separate ID
ID=df.iloc[:,0]
print ID

#all missing values 44, 213,214,840
#df=df.drop(df.columns[[0,1,5,200,202,216 ,222,237,274, 284,305, 325, 342,352, 353, 354,404,466,467,493,44, 213, 840, 1932]], axis=1) 
df=df.drop(df.columns[[0,1,5, 73, 75, 156,157,158,159,166,167,168,169,176,177,178,179,200,202,204,216,217 ,221, 236, 272, 282, 303, 323, 340,350,351,352,402,464,465,491,1932,44, 213, 838]], axis=1)

#discard instances without data/features
df=df.dropna(how='all')
print "Dropped"

df=df.fillna(0)
print "Filled"
#check

#df.to_csv('C:\Users\oswin.frans\Desktop\Springleaf_cleaned.csv')

#need to do dimensionality reduction/feature extraction
#first do PCA and then do feature selection
#Feature Selection http://scikit-learn.org/stable/modules/feature_selection.html
#PCA http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html
#need to define x and y

#csv_file='C:\Users\oswin.frans\Desktop\Python\Springleaf_cleaned.csv'

#df=pd.read_csv(csv_file, sep=',', low_memory=False)
#print "Data read"

collist=df.columns.tolist()
collist.remove("target")
y=df['target']
x=df[collist]

#step I am skipping; creating a cross validation set

#not sure if this helpful if I have time need to test this;
#remove string and long data
x = x.applymap(lambda z: np.nan if isinstance(z, basestring) and z.isspace() else z)
x=x.fillna(-1)

x = x.convert_objects(convert_numeric=True)

#Dimensoniality Reduction

#x = (x - np.mean(x, 0)) / np.std(x, 0) # You need to normalize your data first
#x_norm = (x - x.mean()) / (x.max() - x.min())
x = x.values #returns a numpy array
x_scalzer=StandardScaler().fit_transform(x)
x_norm=pd.DataFrame(x_scalzer)

x_norm=x_norm.fillna(-1)

#print np.any(np.isnan(x_norm))
#print np.all(np.isfinite(x_norm))

#need to remove data with no variation
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
x_Neo=sel.fit_transform(x_norm)
x_Neo=pd.DataFrame(x_Neo)

#print x_Neo.head()

#add datetime back
x_Neo=x_Neo.join(dTimebin)
x_Neo=x_Neo.fillna(-1)

#remove_cols = np.where(np.all(x == np.mean(x, 0), 0))[0]
#x = np.delete(x, remove_cols, 1) 
#n_components == mle
#pca = decomposition.PCA(n_components=='mle') # n_components is the components number after reduction
pca= decomposition.PCA(n_components = 534)
#to check with rule of thumb if things need to be removed
#print pca.fit(x_Neo).explained_variance_

#plt.figure(1, figsize=(4, 3))
#plt.clf()
#plt.axes([.2, .2, .7, .7])
#plt.plot(pca.fit(x_Neo).explained_variance_, linewidth=2)
#plt.axis('tight')
#plt.xlabel('Variables')
#plt.ylabel('explained_variance_')
#plt.show()

x_r = pca.fit(x_Neo).transform(x_Neo)
x_r=pd.DataFrame(x_r)
#print x_r.head()
#print x_r.explained_variance_

o=x_r.join(y)
g=o.join(ID)

#add ID to df

#Feature Selection
#Decided to not Implement currently as 533 seems somewhat manageable, however when using the true dataset this may change
#from sklearn.svm import LinearSVC
#from sklearn.datasets import load_iris
#iris = load_iris()
#X, y = iris.data, iris.target
#X.shape

#X_new = LinearSVC(C=0.01, penalty="l1", dual=False).fit_transform(X, y)
#X_new.shape

#Creating a new file
o.to_csv('C:\Users\oswin.frans\Desktop\Springleaf_preprocessed.csv')
g.to_csv('C:\Users\oswin.frans\Desktop\Springleaf_preprocessed_V2.csv')
