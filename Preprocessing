import pandas as pd
from pandas import *
import numpy as np
#from sklearn.decomposition import PCA
import sklearn.decomposition as deco
from sklearn.feature_selection import VarianceThreshold
from sklearn import preprocessing

csv_file='C:\Users\oswin.frans\Desktop\Python\Springleaf_train.csv'
#tp=pd.read_csv(csv_file, sep=',', iterator=True, chunksize=1000, low_memory=False)
#df = concat(tp, ignore_index=True)
#way to read all data

df=pd.read_csv(csv_file, sep=',', nrows=200)
#read part of the data
#delete rows if a certain column value is zero
#df = df[df.line_race != 0]
#df.to_csv('C:\Users\oswin.frans\Desktop\Springleaf_blaajhk.csv')
print "Data Read"

#first step port these 200 rows back to excel/stata and see how to clean it
#df.to_excel('C:\Users\oswin.frans\Desktop\Check.xlsx',sheet_name='sheet1', index=False)
#clean it
#Recode 1,5; 200, 202,216 ,222,237,274, 284,305, 325, 342,352, 353, 354,404,466,467,493(-1 is missing)
#VAR_218 and VAR_240 are missing
#221, 236, 272, 282, 303, 323, 340,350,351,352,402,464,465,491,1932
torecode=[5,200,202,216 ,221, 236, 272, 282, 303, 323, 340,350,351,352,402,464,465,491,1932]
#torecode=[5,200,202,216 ,222,237,274, 284,305, 325, 342,352, 353, 354,404,466,467,493,1934]
dummyall=pd.DataFrame()
#dummyal= dummyall.fillna(0)

for i in torecode:
	dumcat='dumcat'+str(i)+'.'
	#dumcat2='dumcat'+str(i)+'.2'
	dummy=pd.get_dummies(df.iloc[:,i], prefix=dumcat)
	#df=df.join(dummy.ix[:, dumcat2:])
	#print i
	#print dummy.head()
	dummyall=dummyall.append(dummy)

#print dummyall.head()	
df=df.join(dummyall)
print "Joined"

#timestamp 75, 204, 217

#timestamp with a lot of missing values 73, 156 t/m 159, 166 t/m 169, 176 t/m 179

dTimebin=pd.DataFrame()

dtmz=[73, 75, 156,157,158,159,166,167,168,169,176,177,178,179,204,217]

for i in dtmz:
	dTimebin=dTimebin.join(df.iloc[:,i])

#some misssing values 74, 205 t/m 211

#all missing values 44, 213,214,840
#df=df.drop(df.columns[[0,1,5,200,202,216 ,222,237,274, 284,305, 325, 342,352, 353, 354,404,466,467,493,44, 213, 840, 1932]], axis=1) 
df=df.drop(df.columns[[0,1,5, 73, 75, 156,157,158,159,166,167,168,169,176,177,178,179,200,202,204,216,217 ,221, 236, 272, 282, 303, 323, 340,350,351,352,402,464,465,491,1932,44, 213, 838]], axis=1)

#discard instances without data/features
df=df.dropna(how='all')
print "Dropped"

df=df.fillna(0)
print "Filled"
#check

df.to_csv('C:\Users\oswin.frans\Desktop\Springleaf_cleaned.csv')

#need to do dimensionality reduction/feature extraction
#first do PCA and then do feature selection
#Feature Selection http://scikit-learn.org/stable/modules/feature_selection.html
#PCA http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html
#need to define x and y

#csv_file='C:\Users\oswin.frans\Desktop\Python\Springleaf_cleaned.csv'

#df=pd.read_csv(csv_file, sep=',', low_memory=False)
#print "Data read"

collist=df.columns.tolist()
collist.remove("target")
y=df['target']
x=df[collist]

#step I am skipping; creating a cross validation set

#not sure if this helpful if I have time need to test this;
#remove string and long data
x = x.applymap(lambda z: np.nan if isinstance(z, basestring) and z.isspace() else z)
x=x.fillna(-1)

x = x.convert_objects(convert_numeric=True)

#Dimensoniality Reduction

#x = (x - np.mean(x, 0)) / np.std(x, 0) # You need to normalize your data first
#x_norm = (x - x.mean()) / (x.max() - x.min())
x = x.values #returns a numpy array
#print x.shape
scaler = preprocessing.StandardScaler(with_mean=False)
x_scaler = scaler.fit_transform(X=x.astype(np.float))

#x_scaler = preprocessing.scale(x, axis=0, with_mean=True, with_std=True, copy=True)
#x_scaled = scaler.fit_transform(x)
x_norm = pd.DataFrame(x_scaler)

print x_norm

#need to remove data with no variation
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
x_Neo=sel.fit_transform(x_norm)

#print x_Neo

#add datetime back



#remove_cols = np.where(np.all(x == np.mean(x, 0), 0))[0]
#x = np.delete(x, remove_cols, 1) 

#pca = deco.PCA(n_components=2) # n_components is the components number after reduction
#x_r = pca.fit(x_Neo).transform(x_Neo)
#print ('explained variance (first %d components): %.2f'%(n_components, sum(pca.explained_variance_ratio_)))

#Feature Selection
